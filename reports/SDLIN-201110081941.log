Traceback (most recent call last):
  File "/usr/local/lib/python3.8/site-packages/jupyter_cache/executors/utils.py", line 51, in single_nb_execution
    executenb(
  File "/usr/local/lib/python3.8/site-packages/nbclient/client.py", line 1082, in execute
    return NotebookClient(nb=nb, resources=resources, km=km, **kwargs).execute()
  File "/usr/local/lib/python3.8/site-packages/nbclient/util.py", line 74, in wrapped
    return just_run(coro(*args, **kwargs))
  File "/usr/local/lib/python3.8/site-packages/nbclient/util.py", line 53, in just_run
    return loop.run_until_complete(coro)
  File "/usr/local/Cellar/python@3.8/3.8.8_1/Frameworks/Python.framework/Versions/3.8/lib/python3.8/asyncio/base_events.py", line 616, in run_until_complete
    return future.result()
  File "/usr/local/lib/python3.8/site-packages/nbclient/client.py", line 535, in async_execute
    await self.async_execute_cell(
  File "/usr/local/lib/python3.8/site-packages/nbclient/client.py", line 827, in async_execute_cell
    self._check_raise_for_error(cell, exec_reply)
  File "/usr/local/lib/python3.8/site-packages/nbclient/client.py", line 735, in _check_raise_for_error
    raise CellExecutionError.from_cell_and_msg(cell, exec_reply['content'])
nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
------------------
df = spark.sql(
'''
SELECT *
FROM mordorTable
'''
)
df.show(1, vertical=True)
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mAnalysisException[0m                         Traceback (most recent call last)
[0;32m<ipython-input-3-db71d7186577>[0m in [0;36m<module>[0;34m[0m
[1;32m      5[0m '''
[1;32m      6[0m )
[0;32m----> 7[0;31m [0mdf[0m[0;34m.[0m[0mshow[0m[0;34m([0m[0;36m1[0m[0;34m,[0m [0mvertical[0m[0;34m=[0m[0;32mTrue[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
[0;32m/var/lib/spark/python/pyspark/sql/dataframe.py[0m in [0;36mshow[0;34m(self, n, truncate, vertical)[0m
[1;32m    438[0m         """
[1;32m    439[0m         [0;32mif[0m [0misinstance[0m[0;34m([0m[0mtruncate[0m[0;34m,[0m [0mbool[0m[0;34m)[0m [0;32mand[0m [0mtruncate[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 440[0;31m             [0mprint[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jdf[0m[0;34m.[0m[0mshowString[0m[0;34m([0m[0mn[0m[0;34m,[0m [0;36m20[0m[0;34m,[0m [0mvertical[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    441[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m    442[0m             [0mprint[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jdf[0m[0;34m.[0m[0mshowString[0m[0;34m([0m[0mn[0m[0;34m,[0m [0mint[0m[0;34m([0m[0mtruncate[0m[0;34m)[0m[0;34m,[0m [0mvertical[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;32m/var/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py[0m in [0;36m__call__[0;34m(self, *args)[0m
[1;32m   1302[0m [0;34m[0m[0m
[1;32m   1303[0m         [0manswer[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mgateway_client[0m[0;34m.[0m[0msend_command[0m[0;34m([0m[0mcommand[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0;32m-> 1304[0;31m         return_value = get_return_value(
[0m[1;32m   1305[0m             answer, self.gateway_client, self.target_id, self.name)
[1;32m   1306[0m [0;34m[0m[0m

[0;32m/var/lib/spark/python/pyspark/sql/utils.py[0m in [0;36mdeco[0;34m(*a, **kw)[0m
[1;32m    135[0m                 [0;31m# Hide where the exception came from that shows a non-Pythonic[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[1;32m    136[0m                 [0;31m# JVM exception message.[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 137[0;31m                 [0mraise_from[0m[0;34m([0m[0mconverted[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    138[0m             [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m    139[0m                 [0;32mraise[0m[0;34m[0m[0;34m[0m[0m

[0;32m/var/lib/spark/python/pyspark/sql/utils.py[0m in [0;36mraise_from[0;34m(e)[0m

[0;31mAnalysisException[0m: Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the
referenced columns only include the internal corrupt record column
(named _corrupt_record by default). For example:
spark.read.schema(schema).json(file).filter($"_corrupt_record".isNotNull).count()
and spark.read.schema(schema).json(file).select("_corrupt_record").show().
Instead, you can cache or save the parsed results and then send the same query.
For example, val df = spark.read.schema(schema).json(file).cache() and then
df.filter($"_corrupt_record".isNotNull).count().;
AnalysisException: Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the
referenced columns only include the internal corrupt record column
(named _corrupt_record by default). For example:
spark.read.schema(schema).json(file).filter($"_corrupt_record".isNotNull).count()
and spark.read.schema(schema).json(file).select("_corrupt_record").show().
Instead, you can cache or save the parsed results and then send the same query.
For example, val df = spark.read.schema(schema).json(file).cache() and then
df.filter($"_corrupt_record".isNotNull).count().;

